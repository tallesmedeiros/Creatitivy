# Creatitivy

Seleção de artigos sobre Criatividade e Otimização de Modelos de Liguagem.

1. Brainstorm, then Select: a Generative Language Model Improves Its Creativity Score. Data: 21/11/2022
  . Link: https://openreview.net/forum?id=8HwKaJ1wvl
  
  . Abstract: "Creative problem solving is a crucial ability for intelligent agents. A common method that individuals or groups use to invent creative solutions is to start with a “brainstorming” phase, where many solutions to a problem are proposed, and then to follow with a “selection” phase, where those solutions are judged by some criteria so that the best solutions can be selected. Using the Alternate Uses Task, a test for divergent thinking abilities (a key aspect of creativity) we show that when a large language model is given a sequence of prompts that include both brainstorming and selection phases, its performance improves over brainstorming alone. Furthermore, we show that by following this paradigm, a large language model can even achieve higher than average human performance on the same task. Following our analysis, we propose further research to gain a clearer understanding of what counts as “creativity” in language models."
  
2. A Mathematical Investigation of Hallucination and Creativity in GPT Models. Data: 16/05/2023.

  . Authors: Minhyeok Lee
  
  . Link: https://doi.org/10.3390/math11102320
  
  . Abstract: "In this paper, we present a comprehensive mathematical analysis of the hallucination phenomenon in generative pretrained transformer (GPT) models. We rigorously define and measure hallucination and creativity using concepts from probability theory and information theory. By introducing a parametric family of GPT models, we characterize the trade-off between hallucination and creativity and identify an optimal balance that maximizes model performance across various tasks. Our work offers a novel mathematical framework for understanding the origins and implications of hallucination in GPT models and paves the way for future research and development in the field of large language models (LLMs)."
